[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "spark-map"
version = "0.1.0"
description = "Analyze Spark event logs to identify performance bottlenecks with optional LLM explanations"
readme = "README.md"
license = "MIT"
requires-python = ">=3.9"
authors = [
    { name = "Gabriel", email = "your-email@example.com" }
]
keywords = [
    "spark",
    "apache-spark",
    "performance",
    "profiling",
    "bottleneck",
    "analysis",
    "event-log",
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Environment :: Console",
    "Intended Audience :: Developers",
    "Intended Audience :: System Administrators",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: System :: Monitoring",
    "Topic :: Software Development :: Testing",
    "Typing :: Typed",
]

# Core dependencies - minimal by default
dependencies = [
    "click>=8.0",           # CLI framework
    "rich>=13.0",           # Beautiful terminal output
    "pydantic>=2.0",        # Data validation and schemas
]

[project.optional-dependencies]
# LLM providers - user opts in
ollama = ["ollama>=0.1.0"]
openai = ["openai>=1.0"]
anthropic = ["anthropic>=0.18"]
all-llm = ["ollama>=0.1.0", "openai>=1.0", "anthropic>=0.18"]

# Cloud storage support
s3 = ["boto3>=1.28"]
azure = ["azure-storage-blob>=12.0"]
gcs = ["google-cloud-storage>=2.0"]
all-cloud = ["boto3>=1.28", "azure-storage-blob>=12.0", "google-cloud-storage>=2.0"]

# Development dependencies
dev = [
    "pytest>=7.0",
    "pytest-cov>=4.0",
    "ruff>=0.1.0",
    "mypy>=1.0",
    "pre-commit>=3.0",
]

# Documentation
docs = [
    "mkdocs>=1.5",
    "mkdocs-material>=9.0",
]

[project.scripts]
# This creates the `spark-map` CLI command when installed
spark-map = "spark_map.cli.main:cli"

[project.urls]
Homepage = "https://github.com/yourusername/spark-map"
Documentation = "https://github.com/yourusername/spark-map#readme"
Repository = "https://github.com/yourusername/spark-map"
Issues = "https://github.com/yourusername/spark-map/issues"
Changelog = "https://github.com/yourusername/spark-map/blob/main/CHANGELOG.md"

# ============================================================================
# TOOL CONFIGURATIONS
# ============================================================================

[tool.hatch.build.targets.wheel]
packages = ["spark_map"]

[tool.ruff]
target-version = "py39"
line-length = 100

[tool.ruff.lint]
select = [
    "E",      # pycodestyle errors
    "W",      # pycodestyle warnings
    "F",      # pyflakes
    "I",      # isort
    "B",      # flake8-bugbear
    "C4",     # flake8-comprehensions
    "UP",     # pyupgrade
    "SIM",    # flake8-simplify
]
ignore = [
    "E501",   # line too long (handled by formatter)
]

[tool.ruff.lint.isort]
known-first-party = ["spark_map"]

[tool.mypy]
python_version = "3.9"
strict = true
warn_return_any = true
warn_unused_ignores = true

[[tool.mypy.overrides]]
module = "tests.*"
strict = false
disable_error_code = ["no-untyped-def"]

[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "-v --cov=spark_map --cov-report=term-missing"

[tool.coverage.run]
source = ["spark_map"]
branch = true

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "if TYPE_CHECKING:",
    "raise NotImplementedError",
]
